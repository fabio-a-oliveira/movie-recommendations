---
title: "A movie rating prediction model"
author: "Fabio A Oliveira"
date: "21/09/2020"
output: pdf_document
toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, include = TRUE, fig.align = 'center', out.width = "70%", out.height = "70%")
options(width = 60)
```

```{r install and load libraries, include = FALSE, results = 'hide'}

# Install and load libraries

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(Matrix)) install.packages("Matrix", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if (!("fs" %in% installed.packages())) install.packages("fs")

library(tidyverse)
library(data.table)
library(lubridate)
library(knitr)
library(Matrix)
library(caret)

# Expand available memory

memory.limit(size = 20000)
gc()

```

```{r load workspace, include = FALSE, results = 'hide'}

# this chunk loads the results from the HX9_MovieLens_Main.R script. You need to either clone the repo to your machine in order to have these files available in the \results folder or run the script yourself.

load(file.path("results","small_objects.RData"))

edx <- fs::dir_ls("results") %>% str_subset("(?<!_)edx_\\d{3}") %>% map_dfr(read_csv,col_types=cols())
ratings <- fs::dir_ls("results") %>% str_subset("(?<!_)ratings_\\d{3}") %>% map_dfr(read_csv,col_types=cols())
test <- fs::dir_ls("results") %>% str_subset("(?<!_)test_\\d{3}") %>% map_dfr(read_csv,col_types=cols())
train <- fs::dir_ls("results") %>% str_subset("(?<!_)train_\\d{3}") %>% map_dfr(read_csv,col_types=cols())
users <- fs::dir_ls("results") %>% str_subset("(?<!_)users_\\d{3}") %>% map_dfr(read_csv,col_types=cols())
validation <- fs::dir_ls("results") %>% str_subset("(?<!_)validation_\\d{3}") %>% map_dfr(read_csv,col_types=cols())

predictions <- 
  list(train = fs::dir_ls("results") %>% str_subset("(?<!_)predictions_train_\\d{3}") %>% map_dfr(read_csv,col_types=cols()),
       test = fs::dir_ls("results") %>% str_subset("(?<!_)predictions_test_\\d{3}") %>% map_dfr(read_csv,col_types=cols()),
       validation = fs::dir_ls("results") %>% str_subset("(?<!_)predictions_validation_\\d{3}") %>% map_dfr(read_csv,col_types=cols()),
       `sample users` = fs::dir_ls("results") %>% str_subset("(?<!_)predictions_sample_users_\\d{3}") %>% map_dfr(read_csv,col_types=cols()))

```

***

# Introduction

This report describes the construction of a statistical model that predicts movie ratings, based on the _MovieLens 10M_ dataset provided by the [GroupLens](https://grouplens.org/) research lab at the University of Minnesota. This version of the dataset has been released in 2009 and contains over 10 million ratings given by around 70 thousand individual users to over 10 thousand different movies, with each entry containing identification numbers for the user and movie, as well as the movie title and release year, its genres and a timestamp identifying when the rating was given. The dataset has been used regularly by the Data Science community in teaching and in the development of machine learning models.

Before attempting to construct any statistical model, some basic data wrangling is performed, followed by an exploratory data analysis, in order to identify trends in the data that may be later used to construct predictions.

An additive approach has been used to construct the final model, in the sense that a number of increasingly complex models has been created so that, at each step, an additional term accounts for patterns identified in the residuals from the previous step. With this approach, the variable under evaluation at each step can be analyzed individually, and the correlation between different predictors is not a point of concern.

For the purposes of this project, the full 10M dataset has been partitioned into three sets: 

1. A _train_ set, from where patterns are identified and quantified in order to build the statistical model; 
2. A _test_ set, where different parameters are tested and tuned in order to find optimal performance and 
3. A _validation_ set, which will not be used for training or tuning of parameters, and where the models will be evaluated. 

The ultimate goal of the project is to create a model capable of predicting ratings in the _validation_ set as precisely as practical. The _Root Mean Squared Error_ (RMSE) of the predictions compared to the actual ratings is reported as a performance metric. The RMSE obtained after application of the complete model to the _validation_ set was 0.843.

A Shiny Web App with a minimalist version of the final model has been created for illustrative purposes and can be accessed at <https://fabio-a-oliveira.shinyapps.io/MovieRecommenderApp/>.

The GitHub page for this project is <https://github.com/fabio-a-oliveira/movie-recommendations>.

This analysis is part of the capstone project for the Data Science Professional Certificate offered by HarvardX and hosted on edX. More information can be found at <https://www.edx.org/professional-certificate/harvardx-data-science>.

***

# Methods/analyses

The project has been developed according to a sequence of steps:

1. Data preparation: during this step, the MovieLens 10M dataset is downloaded programmaticaly from the GroupLens website and partitioned into the _train_, _test_, and _validation_ sets. Care is taken to ensure that every user and movie are represented in the _train_ set (so that the model is not required to make predictions for users and movies to which it has not been exposed during training). Separate data frames are created with statistics for each individual users, movies and movie genres, so that information gathered during the following steps can be store and accessed easily. Empty data frames are also created to store functions to apply each of the models, its predictions for each of the datasets and the results.
2. Exploratory data analysis: during this step, the _train_ set is explored to identify relevant features, how the different variables are related, and which characteristics can potentially be used during the modeling step in order to predict ratings. This step is also used to get familiarity with the data.
3. Modeling: during this step, increasingly complex models are developed, trained and tested to account for the influence of the factors identified during the exploratory data analysis on the ratings given to movies by each user. These models are created so that the simplest connections are accounted for sooner (how does a single predictor relate to the ratings?), followed by more complex phenomenons (how do combinations of predictors relate to the ratings?). The final model includes an _Item-Based Collaborative Filter_ (IBCF), in which the predictions for how each individual user rates each movie takes into consideration how the user has previously rated similar movies.

Each of these steps is explained in more details in the corresponding section of this report.

Additionally, due to the size of the original dataset used in this analysis, significant challenges related to computing performance were tackled. Constant care was taken to avoid using unnecessary memory. Nevertheless, certain sections of the code required a command to increase the amount of memory available to R with the `memory.limit(size = 16000)` command. During the creation of the model with the IBCF in particular, the calculations required the use of the `Matrix` package for dealing with sparse matrices and the implementation of custom functions to identify the similarity between different movies in a memory-efficient manner.

The R code for this project is divided into two different files. All of the data preparation and the modeling are done in the `HX9_MovieLens_Main.R` script. This script prepares the data and does all of the calculations necessary for the development of the models, creates functions to make predictions according to each model, and calculates the performance for each model. It then saves the resulting objects into files that are loaded by the `HX9_MovieLens_Report.RMD` script (the R Markdown script that generates this report) to create all the tables and graphics in this report.

***

## Data preparation

A series of adjustments to the original _MovieLens 10M_ dataset is necessary before we begin analysis and modeling. These adjustments are described below:  
  
#### 1. The original data is partitioned into the _train_, _test_, and _validation_ datasets.   

As a first step, the original data is partitioned into the _edx_ and _validation_ data frames at a 0.9/0.1 proportion. The _edx_ object is then partitioned into the _train_ and _test_ sets at a 0.8/0.2 proportion.

The table below shows how the full set of entries from the _MovieLens 10M_ dataset is distributed among the three different sets used in this project:

```{r size of the partitions}

full <- bind_rows(edx,validation)

data.frame(Object = c("train", "test", "validation"),
           ratings = c(nrow(train), nrow(test), nrow(validation)),
           users = c(n_distinct(train$userId), n_distinct(test$userId), n_distinct(validation$userId)),
           movies = c(n_distinct(train$movieId), n_distinct(test$movieId), n_distinct(validation$movieId))) %>% 
  mutate(proportion_ratings = round(ratings / nrow(full),2) * 100,
         proportion_users = round(users / n_distinct(full$userId),2) * 100,
         proportion_movies = round(movies / n_distinct(full$movieId),2) * 100) %>% 
  transmute(Object,
            "Number of Ratings (% of total)" = paste0(ratings," (",proportion_ratings,"%)"),
            "Number of Users (% of total)" = paste0(users," (",proportion_users,"%)"),
            "Number of Movies (% of total)" = paste0(movies," (",proportion_movies,"%)")) %>% 
  kable(caption = "Entries in the different sets")

rm(full)

```

It is relevant to notice that the _train_ set contains entries for all the individual users and movies, so the different models are not required to predict ratings for users and movies with which they have had not previous training.

A glimpse at a sample of these objects reveals what information is available and what further adjustments are necessary:

```{r basic information on the full dataset}

options(width = 95)
set.seed(20200921)
bind_rows(edx,validation) %>% 
  filter(str_length(title) < 30 & str_length(genres) < 20) %>%
  sample_n(15)

```

After a quick glimpse at the dataset, we see that the `timestamp` column is formatted in a manner that is difficult to read. The `title` column contains both the movie title and its year of release. Additionally, the `genres` column contains all the genres associated with the movie, separated by a "|" character. Some basic data wrangling is necessary to have this information in usable formats. However, in order to preserve these objects as close as possible to the original data, any manipulation and formatting of the data is done in the objects created in the next step.

The choice for doing training and testing with a single split of the _edx_ data (as opposed to performing k-fold cross-validation) has been motivated by simplicity and saving time in code execution. With this approach, statistics can be drawn directly from the _train_ set and performance for different parameters can be tested directly on the _test_ set, without the need to average results over multiple folds. The fact that there are millions of observations in both sets guarantees that the outcomes are robust to random variability and mitigate the risk of overtraining.  
  
#### 2. Objects are created to hold data pertinent to individual entries on the datasets   

Several R objects are created to hold relevant information that will be used for analysis, modeling and holding results during the project. Basic summary statistics and some necessary data wrangling identified in the analysis of the _train_, _test_, and _validation_ sets are also performed and saved to the new objects.

* The `users` object contains a summary with the average rating and number of ratings for each individual user in the form of a data frame;
```{r sample of users object}
users %>% select(userId, `average rating`, `number of ratings`) %>% sample_n(6) %>% as.data.frame()
```

* The `movies` object contains a summary with the average rating and number of ratings for each individual movie in the form of a data frame; 

```{r sample of movies object}
options(width = 110)
movies %>% select(movieId, title, year, genres, `average rating`, `number of ratings`) %>% 
  filter(str_length(title) < 20 & str_length(genres) < 20) %>% 
  sample_n(6) %>% as.data.frame()
```

* The `genres` object contains both a data frame with each individual movie genre associated with each movie (in "long" format) and a data frame with columns for each movie genre indicating whether each movie is associated with each particular movie (in "wide" format), in the form of a list;

```{r sample of the genres object}
options(width = 90)
genres$long %>% as.data.frame() %>% head(10)
genres$wide %>% as.data.frame() %>% head(10)
```

* The `models` object is an empty list of functions, which will be filled during the modeling phase with functions calculating predictions for each different model;
* The `parameters` object is an empty list, which will contain parameters required for the application of the models calculated during the modeling phase;
* The `results` object is an empty list, which will be filled with results acchieved by each of the models and
* The `predictions` object is a list containing empty data frames that will be filled with the RMSE results obtained by each model on the _train_, _test_, and _validation_ sets.

During the creation of the models, additional information pertinent to each entry of these data frames is included.  
  
#### 3. Creation of functions   

In order to facilitate calculations during the subsequent phases, some R functions are created to perform tasks that are repeated multiple times:

* The `RMSE()` function calculates the Root Mean Squared Error between two supplied vectors;
* The `limitRange()` function introduces a saturation to a supplied vector, ensuring that each entry is within the interval determined by the `min` and `max` arguments and
* The `sparse.colCenter()`, `sparse.colMeans()`, `sparse.colSums()` and `sparse.correlationCommonRows()` functions perform numerous sparse matrix manipulation tasks in a memory-conscious manner and will be described in more details in the section dedicated to the model with collaborative filtering.

***

## Exploratory Data Analysis

Before we tackle the prediction problem, we begin by doing a thorough exploratory data analysis, with the goal of identifying trends and features of the data that might be useful in understanding it and possibly in constructing a prediction algorithm.

Five aspects will be explored in details in this section:

1. Distribution of ratings
2. Individual movies
3. Individual users
4. Movie genres
5. Movie year of release and date of rating

At this stage, no attempt will be made to predict the actual ratings, but trends identified during the exploratory data analysis will be later used as criteria to build prediction models during the modeling section.

### Global distribution of ratings

We begin by looking at how the ratings are distributed in the _train_ set.

```{r histogram of ratings}

train %>% 
  ggplot(aes(x = rating)) +
  geom_histogram(aes(y = stat(count) / sum(stat(count))), 
                 binwidth = .5, color = 'black', fill = 'blue', size = 1) +
  geom_vline(xintercept = mean(train$rating), color = 'red', size = 2) +
  scale_x_continuous(breaks = seq(0.5,5,0.5), minor_breaks = NULL) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Histogram of ratings in the train set",
       x = "Rating",
       y = "Percentage of ratings") +
  annotate(geom = "curve", x = 3, y = .285, xend = 3.45, yend = .275, curvature = -.3, arrow = arrow(length = unit(2,'mm')), color = 'red') +
  annotate(geom = "label", x = 2.6, y = .275, label = "average \n rating: 3.51", hjust = "center")

```

We see that the ratings range from 0.5 to 5.0 in increments of 0.5, which is rather typical of a rating system based on number of stars. The histogram also shows that the most common ratings are 4 and 3, and that most integer ratings are more common than non-integer ratings.


```{r boxplot of ratings from sample}

movies %>% 
  select(movieId,`average rating`) %>% 
  sample_n(30) %>% 
  left_join(select(train,movieId,rating), by = "movieId") %>% 
  ggplot(aes(group = reorder(movieId,`average rating`), y = rating)) +
  geom_boxplot(fill = 'green2') +
  geom_hline(aes(yintercept = median(movies$`average rating`), color = "median movie \n rating: 3.27"), size = 2) +
  labs(title = "Boxplot of ratings for a sample of 30 movies",
       x = "Movie",
       y = "Ratings") +
  scale_x_continuous(breaks = NULL, minor_breaks = NULL) +
  scale_y_continuous(breaks = seq(0.5,5,0.5), minor_breaks = NULL) +
  scale_color_manual(values = 'red') +
  theme(legend.position = "right",
        legend.title = element_blank())

```

This boxplot of the ratings for 30 randomly selected movies shows that the Inter-Quartile Range (IQR) is typically the interval of +/- 0.5 points around the median. However, the majority of movies do have ratings that cover most of the 0.5 - 5 points scale.  

### Analysis of individual movies

We begin the exploration of the characteristics of individual movies by looking at the table of the most popular movies, i.e. the ones with the most number of ratings.


```{r most popular movies}

movies %>% 
  select(movieId, title, `average rating`, `number of ratings`) %>% 
  top_n(10, wt = `number of ratings`) %>% 
  arrange(desc(`number of ratings`)) %>% 
  rename("Title" = "title",
         "Average Rating" = "average rating",
         "Number of Ratings" = "number of ratings") %>% 
  kable(digits = 2, caption = "Top 10 Popular Movies")

```

It is also interesting to see which movies have the highest average ratings.

```{r best rated movies}

movies %>% 
  select(movieId, title, `average rating`, `number of ratings`) %>% 
  top_n(10, wt = `average rating`) %>% 
  arrange(desc(`average rating`)) %>% 
  rename("Title" = "title",
         "Average Rating" = "average rating",
         "Number of Ratings" = "number of ratings") %>% 
  kable(digits = 2, caption = "Top 10 Best Rated Movies")

```

From this previous table, we notice that the movies at the top of the list all have very low number of ratings. That indicates that they high averages are most likely a product of random variability. When we create a list of the top 10 movies arranged by their ratings, but first filter out movies with less then 100 individual ratings, we get a more reasonable result:


```{r best rated movies with over 100 ratings}

movies %>% 
  filter(`number of ratings` > 100) %>% 
  select(movieId, title, `average rating`, `number of ratings`) %>% 
  top_n(10, wt = `average rating`) %>% 
  arrange(desc(`average rating`)) %>% 
  rename("Title" = "title",
         "Average Rating" = "average rating",
         "Number of Ratings" = "number of ratings") %>% 
  kable(digits = 2, caption = "Top 10 Best Rated Movies (with over 100 ratings)")

```

A histogram of the average movie ratings shows that movie averages are most common in the vicinity of 3.5. However, the overall average is actually around 3.19, which indicates that the data is skewed to the right. This makes sense, since the upper limit of the interval of possible ratings is closer to the average, so movies better rated than the average are more concentrated.

```{r histogram of ratings per movies}

movies %>% 
  ggplot(aes(x = `average rating`)) +
  geom_histogram(aes(y = stat(count) / sum(stat(count))), 
                 binwidth = .1, color = 'black', fill = 'blue', size = 1) +
  geom_vline(xintercept = mean(movies$`average rating`), color = 'red', size = 2) +
  scale_x_continuous(breaks = seq(0.5,5,0.5), minor_breaks = NULL) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Histogram of average ratings per individual movie",
       x = "Rating",
       y = "Percentage of movies") +
  annotate(geom = "curve", x = 2.6, y = .08, xend = 3.12, yend = .085, curvature = -.3, arrow = arrow(length = unit(2,'mm')), color = 'red') +
  annotate(geom = "label", x = 2.1, y = .08, label = "average movie \n rating: 3.19", hjust = "center")

```

The following histogram shows the distribution of the total number of ratings. We see that there is a large concentration in the low numbers, with an average of 674 reviews per movie, while some have over 20 thousand ratings.

This concentration of ratings into few movies has a dramatic effect on the average. The actual median number of ratings is much lower, at 98, with the IQR of 24 to 453 reviews.


```{r histograms of number of ratings per movie}

movies %>% 
  ggplot(aes(x = `number of ratings`)) +
  geom_histogram(breaks = seq(0,30000,2500), fill = 'blue', color = 'black', size = 2) +
  geom_vline(xintercept = mean(movies$`number of ratings`), size = 2, color = 'red') +
  scale_x_continuous(breaks = seq(0,30000,2500), minor_breaks = NULL, limits = c(0,30000)) +
  scale_y_continuous(trans = "sqrt", limits = c(0,12000), breaks = c(1000,seq(0,10000,2000)), minor_breaks = NULL) +
  annotate(geom = "curve", curvature = .2, x = 4500, y = 11000, xend = 1000, yend = 11000, color = 'red', arrow = arrow(length = unit(2,'mm'))) +
  annotate(geom = "label", x = 7500, y = 10000, label = "average number \n of ratings per \n movie: 674") +
  labs(title = "Histogram of number of ratings per movie",
       x = "Number of ratings",
       y = "Number of movies") +
  theme(axis.text.x = element_text(angle = 45))

```

A scatter plot of the average rating per number of ratings for the entire `movies` object illustrates some of these points. We see in the figure below that the top 10 best rated movies are all concentrate into a single region and are obviously outliers. There is a clear positive correlation between movie popularity and rating, and the most popular movies are indeed well rated. However, the top 10 best rated movies with over 100 ratings are somewhat evenly distributed in the range between roughly 2 and 22 thousand ratings.

```{r movie ratings vs popularity}

movies %>% 
  ggplot(aes(x = `number of ratings`, y = `average rating`)) +
  geom_point(color = 'gray') +
  geom_point(data = top_n(movies,10,`average rating`), color = 'red', size = 3.3) +
  geom_point(data = top_n(filter(movies, `number of ratings` > 100),10,`average rating`), color = 'blue', size = 3.3) +
  geom_point(data = top_n(movies,10,`number of ratings`), color = 'green2', size = 3) +
  geom_smooth(method = 'lm', formula = y ~ x, se = FALSE, mapping = aes(color = 'linear regression line'), size = 1, linetype = "dashed") +
  scale_x_continuous(trans = "sqrt", breaks = c(0,2000,seq(0,30000,5000))) +
  labs(title = "Number of ratings vs Average rating",
       x = "Number of ratings",
       y = "Average rating") +
  annotate(geom = "curve", color = "red", xend = 10, yend = 5, x = 200, y = 4.8, curvature = .2, arrow = arrow(length = unit(2,"mm"))) +
  annotate(geom = "text", label = "top 10 best \n rated movies", x = 300, y = 4.5, size = 3.5) +
  annotate(geom = "curve", color = "blue", xend = 2500, yend = 4.4, x = 4000, y = 4.8, curvature = .2, arrow = arrow(length = unit(2,"mm"))) +
  annotate(geom = "text", label = "top 10 best rated \n movies (100+ ratings)", x = 7100, y = 4.9, size = 3.5) +
  annotate(geom = "curve", color = "green", xend = 22000, yend = 3.5, x = 20000, y = 2.8, curvature = .2, arrow = arrow(length = unit(2,"mm"))) +
  annotate(geom = "text", label = "top 10 most \n popular movies", x = 20000, y = 2.5, size = 3.5) +
  theme(legend.title = element_blank())

```

Finally, we look at the cumulative proportion of ratings, with all movies order from most to least popular. The figure below shows that, from a total of 10677 movies, only around 500 are necessary to account for half the individual ratings, while around 2800 movies account for 90% of all the ratings.


```{r cumulative number of ratings}

movies %>% 
  arrange(desc(`number of ratings`)) %>% 
  mutate("cumulative sum" = cumsum(`number of ratings`),
         "cumulative proportion" = `cumulative sum` / sum(`number of ratings`),
         "sequential number" = 1:nrow(movies)) %>% 
  ggplot(aes(x = `sequential number`, y = `cumulative proportion`)) +
  geom_line(size = 2, color = "blue") +
  scale_x_continuous(breaks = c(seq(0,10000,1000), nrow(movies)), minor_breaks = NULL) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Cumulative percentage of ratings",
       x = "Movies \n (in order of popularity)",
       y = "Cumulative percentage of total ratings")

```

From the analysis of individual movies we see that there is a clear connection between popularity and how a movie is rated on average. We also see that there is considerable random noise in the set of movies that are more obscure, with very few ratings. Finally, we see that the ratings are very concentrated in a reduced set of very popular movies.

### Analysis of individual users

We now turn our attention to the behavior of individual users. First, we look at the distribution of user's individual mean ratings.

```{r histogram of average ratings per user}

users %>% 
  ggplot(aes(x = `average rating`)) +
  geom_histogram(aes(y = stat(count / sum(count))),
                 color = 'black', fill = 'blue', size = 1, bins = 60) +
  geom_vline(xintercept = mean(users$`average rating`), size = 2, color = 'red') +
  scale_y_continuous(labels = scales::percent, limits = c(0,.09), breaks = seq(0,.1,.01), minor_breaks = NULL) +
  scale_x_continuous(breaks = seq(0,5,0.5), minor_breaks = NULL) +
  labs(title = "Average ratings per individual user",
       x = "Rating", y = "Percentage of users") +
  annotate(geom = "curve", color = 'red', xend = 3.52, yend = .081, x = 3.1, y = .085, curvature = -.2, arrow = arrow(length = unit(2,'mm'))) +
  annotate(geom = 'label', x = 2.6, y = .085, label = 'average user \n mean: 3.61')

```

The figure shows that average ratings are distributed according to a roughly bell-shaped curve, centered at 3.61. The curve is slightly skewed to the right, which agrees with the fact that the average is closer to the top limit of 5 than the bottom limit of 0.5.

The frequency at which users rate is depicted in the histogram below: 

```{r histogram of number of ratings per user}

users %>% 
  ggplot(aes(x = `number of ratings`)) +
  geom_histogram(breaks = seq(0,6000,250), color = 'black', fill = 'blue', size = 2) +
  geom_vline(xintercept = mean(users$`number of ratings`), color = 'red', size = 2) +
  scale_y_continuous(trans = 'sqrt', limits = c(0,80000), breaks = c(0,10000,20000,40000,60000)) +
  scale_x_continuous(breaks = seq(0,6000,250), minor_breaks = NULL) +
  theme(axis.text.x = element_text(angle = 45)) +
  labs(title = "Number of ratings per user", x = "Number of ratings", y = "Number of users") +
  annotate(geom = "curve", curvature = .2, xend = 240, yend = 71000, x = 700, y = 72000, color = 'red', arrow = arrow(length = unit(2, 'mm'))) +
  annotate(geom = "label", x = 1700, y = 72000, label = "average number \n of ratings per user: 103")
```

We see from the histogram that the vast majority of users have rated between 0 and 250 movies, with the average at 103. However, some users have rated considerably more often than that, with 300 users having rated more than one thousand movies, and 2 having rated over five thousand.

Some basic statistics for the number of ratings per user are shown in the table below:

```{r quantiles for number of ratings per user}

users %>% summarise("0% (min.)" = min(`number of ratings`),
                    "25% (1st qu.)" = quantile(`number of ratings`, .25),
                    "50% (median)" = quantile(`number of ratings`, .50),
                    "Mean" = mean(`number of ratings`),
                    "75% (3rd qu.)" = quantile(`number of ratings`, .75),
                    "100% (max.)" = max(`number of ratings`)) %>% 
  kable(caption = "Mean and quantiles for individual user's number of ratings")

```

Next, we investigate whether users rate consistently across different movies. The following plot shows the distribution of each user's rating standard deviation. Users are grouped according to their mean rating.

```{r standard deviation of users ratings}

train %>%
  group_by(userId) %>%
  summarise("mean rating" = mean(rating), "sd rating" = sd(rating), "number of ratings" = n(), .groups = 'drop') %>%
  # filter(`number of ratings` > 20) %>% 
  mutate("rating group" = cut(`mean rating`, breaks = seq(.5,5,.5), include.lowest = TRUE),
         "mean sd" = mean(`sd rating`)) %>%
  ggplot(aes(x = `rating group`, y = `sd rating`)) +
  geom_boxplot(fill = "green2") +
  geom_hline(aes(yintercept = first(`mean sd`)), color = 'red', size = 1, alpha = .5) +
  labs(title = "Standard deviation of ratings according to user average rating", x = "User groups according to each user's average rating",
       y = "Standard deviation of each user's ratings") +
  annotate(geom = "label", x = 1, y = 1.2, label = "mean per \n user rating \n SD: 0.958", size = 3)

```

The figure shows that users tend to be consistent, with the mean standard deviation at 0.958. As one would expect, this standard deviation is lower for users with average ratings closer to the extremes of the scale.

Finally, we look at the cumulative percentage of ratings. The figure below shows this cumulative percentage, with users ordered according to their total number of ratings.

```{r cumulative ratings per user}

users %>% 
  arrange(desc(`number of ratings`)) %>% 
  mutate("cumulative ratings" = cumsum(`number of ratings`),
         "cumulative proportion" = cumsum(`number of ratings`) / sum(`number of ratings`),
         "seq" = 1:nrow(users)) %>% 
  ggplot(aes(x = seq, y = `cumulative proportion`)) +
  geom_line(color = 'blue', size = 2) +
  scale_y_continuous(labels = scales::percent, breaks = seq(0,1,.25)) +
  scale_x_continuous(breaks = c(seq(0,65000,5000), nrow(users)), minor_breaks = NULL) +
  labs(title = "Cumulative percentage of ratings", x = "Users \n (in order of number of ratings)", y = "Cumulative percentage of total ratings")

```

The curve shows that, while there is a concentration of the total number of ratings in the most active users, this concentration is not as pronounced as was observed for the most popular movies. The table below compares how much of the full sets of movies and users are required to account for 50% and 90% of the total ratings.


```{r comparison of concentration}

cumulative_users <-
  users %>% 
  arrange(desc(`number of ratings`)) %>% 
  transmute("proportion of users" = 1:nrow(users) / nrow(users),
            "cumulative proportion" = cumsum(`number of ratings`)/sum(`number of ratings`))

cumulative_movies <-
  movies %>% 
  arrange(desc(`number of ratings`)) %>% 
  transmute("proportion of movies" = 1:nrow(movies) / nrow(movies),
            "cumulative proportion" = cumsum(`number of ratings`)/sum(`number of ratings`))

data.frame("Set" = c("movies", "users"),
           "Total" = c(nrow(movies), nrow(users)),
           "p50" = c(cumulative_movies$`proportion of movies`[which.min(abs(cumulative_movies$`cumulative proportion` - 0.5))],
                                   cumulative_users$`proportion of users`[which.min(abs(cumulative_users$`cumulative proportion` - 0.5))]),
           "p90" = c(cumulative_movies$`proportion of movies`[which.min(abs(cumulative_movies$`cumulative proportion` - 0.9))],
                                   cumulative_users$`proportion of users`[which.min(abs(cumulative_users$`cumulative proportion` - 0.9))])) %>%
  mutate(p50 = scales::percent(p50), p90 = scales::percent(p90)) %>% 
  # rename("Percentage to account for 50% of all ratings" = "p50", "Percentage to account for 90% of all ratings" = "p90") %>% 
  kable(caption = "Concentration of total ratings in most popular movies and most active users", align = 'c',
        col.names = c("Variable", "Total", "Percentage to account for 50% of all ratings", "Percentage to account for 90% of all ratings"))

rm(cumulative_users, cumulative_movies)

```

The table confirms what was observed in the plots: there is a much higher concentration of the total ratings into the most popular movies than there is into the most active users. This indicates that statistical conclusions drawn from a reduced list of popular movies may prove to have more significance in predicting results for the full set than statistics drawn from a reduced list of most active users.

From this analysis of the individual users, we conclude that the typical user is rather consistent in giving ratings, with the average standard deviation at around 0.95 and the mean user average at 3.61. We also see that the typical user is not very active, with half of the users having less than 50 ratings in the _train_ set. Finally, there is a clear concentration of the total number of ratings into the most active users, but it is not nearly as pronounced as observed across movies.  
  
  
### Analysis of movie genres

Upon inspection of the `genres` column presented in the data, we see that each entry is comprised of a string of characters with all genres in which a movie fits, separated by "|". Below we see a sample with 12 entries with relevant columns of the `movies` object:

```{r sample with movie genres}

movies %>% filter(str_length(title) < 30) %>% sample_n(12) %>% select(movieId,title,genres)

```

After some data manipulation, we see that the `genres` variable contains 19 different genres, which are counted and arranged by number of appearances in the table below:

```{r popularity of genres}

movies %>% 
  left_join(genres$long, by = "genres") %>% 
  group_by(genre) %>%  
  summarise("number of movies" = n(), "average rating" = mean(`average rating`), .groups = 'drop') %>% 
  arrange(desc(`number of movies`)) %>% 
  transmute("#" = row_number(), genre, `number of movies`, `average rating`) %>% 
  kable(caption = "Movie genres arranged by number of movies", col.names = c("#", "Genre", "Number of Movies", "Average Rating"), align = 'c', digits = 2)

```

We see that "Drama" is the most common movie genre. The table also shows that there is a relevant difference between average movie ratings across different genres. We can see the distribution of these average ratings in the plot below:

```{r average ratings per movie genre}

movies %>% 
  left_join(genres$long, by = "genres") %>% 
  ggplot(aes(x = reorder(genre, `average rating`), y = `average rating`)) +
  geom_boxplot(fill = 'green2') +
  geom_hline(size = 1, aes(color = 'Average movie \n rating: 3.51', yintercept = mean(movies$`average rating`))) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(title = "Average movie rating per genre", x = "Movie genre", y = "Average rating") +
  theme(legend.title = element_blank())


```

As indicated in the previous table, all genres other than "IMAX" have over 100 individual movies, which suggests that the relation between movie genres and average movie ratings is an actual feature of the data and not due to random variability.

We also note that each movie is not associated with a single genre, but rather a collection of genres. In fact, 4 movies classified with 7 or 8 different genres:

```{r movie with most genres}

movies %>% 
  left_join(genres$long, by = "genres") %>% 
  group_by(movieId,title) %>% 
  summarise("number of genres" = n(), .groups = 'drop') %>% 
  arrange(desc(`number of genres`)) %>% 
  left_join(select(movies,movieId,genres), by = "movieId") %>% 
  filter(`number of genres` >= 7) %>% 
  select(title,`number of genres`, genres) %>% 
  kable(caption = "Movies with 6+ different genres", col.names = c("Title","# Genres","Genres"))

```

Some genres are more frequently found together. We calculate the correlations between the columns in the `genres$wide` data frame and show the highest and lowest pairs to demonstrate:

```{r highest correlations between genres}

genres$wide %>% 
  column_to_rownames("genres") %>% 
  as.matrix() %>% 
  cor() %>% 
  as.data.frame() %>% 
  rownames_to_column("genre1") %>% 
  pivot_longer(cols = -"genre1", values_to = "correlation", names_to = "genre2") %>% 
  filter(genre1 < genre2) %>% 
  arrange(desc(correlation)) %>% 
  unite(col = "pair", genre1, genre2, sep = " | ") %>% 
  head(10) %>% 
  kable(caption = "Movie genre pairs with highest correlation", col.names = c("Pair","Correlation"))

```

Not surprisingly, there is positive correlation between genres like "animation" and "children", as well as "mystery" and "thriller" or "action" and "adventure". Similar results appear when we look at the bottom of the table, with the least correlated genres:


```{r lowest correlations between genres}

genres$wide %>% 
  column_to_rownames("genres") %>% 
  as.matrix() %>% 
  cor() %>% 
  as.data.frame() %>% 
  rownames_to_column("genre1") %>% 
  pivot_longer(cols = -"genre1", values_to = "correlation", names_to = "genre2") %>% 
  filter(genre1 < genre2) %>% 
  arrange(correlation) %>% 
  unite(col = "pair", genre1, genre2, sep = " | ") %>% 
  head(10) %>% 
  kable(caption = "Movie genre pairs with lowest correlation", col.names = c("Pair","Correlation"))

```

As you would imagine, certain pairs are not found often (though I suspect writing a children's thriller or an action musical would be an interesting challenge for a movie writer).

From this analysis of the movies genres, we conclude that there is a large difference in the frequency with which individual genres are used to describe movies, with "Drama" and "Comedy" being the most frequent. There is also a connection between the movie genres and the average rating, with movies tagged with "Horror" averaging 2.80, while ones tagged with "Film-Noir" average 3.72. We also note a significant correlation between certain pairs of genres that are often found together.  
  
### Analysis of time

As a final step in the exploratory data analysis, we look at the distribution of the year of release of each movie and how the average ratings are distributed according to the year of release. We also investigate how the number of years since the release of each movie at the time of rating influences the rating.

The figure below contains a depiction of the number of movies released at each year and the average rating for movies released at each year:

```{r average rating per year of release}

movies %>% 
  group_by(year) %>% 
  summarise("Number of movies" = n(), "Average rating" = mean(`average rating`), .groups = 'drop') %>%
  pivot_longer(cols = -`year`, names_to = "variable", values_to = "value") %>% 
  ggplot(aes(x = year, y = `value`)) + 
  geom_line(color = 'blue', size = 1) +
  facet_wrap(~ variable, scales = "free") +
  scale_x_continuous(breaks = seq(1900,2020,10), minor_breaks = NULL) +
  labs(title = "Average movie rating and number of movies released per year", x = "Year", y = "") +
  theme(axis.text.x = element_text(angle = 45))

```

We see that the number of movies in the dataset increases dramatically in the late 70s, as well as in the early 90s. We also see that the average movie rating has a strong negative correlation with the year of release.

It is also interesting to look at the average ratings as a function of the number of years passed since the release of the movie at the time of each rating. The plot below shows this information:

```{r average rating per years since release}

ratings %>% 
  select(userId,movieId,rating,date) %>% 
  left_join(select(movies,movieId,year), by = "movieId") %>% 
  mutate("years since release" = if_else(year(date) - year > 0,
                                         year(date) - year, 0)) %>% 
  group_by(`years since release`) %>% 
  summarise("average rating" = mean(rating), .groups = 'drop') %>% 
  ggplot(aes(x = `years since release`, y = `average rating`)) +
  geom_line(color = 'blue', size = 1.5) +
  geom_hline(yintercept = mean(ratings$rating), color = 'red', size = 2, alpha = .5) +
  scale_x_continuous(trans = "reverse", breaks = seq(0,100,10)) +
  labs(title = "Average rating per years since movie release", x = "Years since release", y = "Average rating") +
  annotate(geom = "label", color = 'blue', label = "Average rating", x = 35, y = 3.9) +
  annotate(geom = "label", color = 'red', label = "Global average", x = 45, y = 3.47)

```

Again, we see that there is a strong connection between the movie age at the time of rating and the value of the rating, demonstrating a tendency of good movies to survive the test of time.

From this analysis, we conclude that there is a prevalence of newer movies in the dataset, but that older movies (be it according to year of release of age at the time of rating) tend to be better rated.

***

## Modeling

We now proceed to use the hints provided by the exploratory data analysis to construct models that predict movie ratings given by users. We begin by constructing a reference model in which all predictions are the global average rating calculated on the _train_ set. Then, we create increasingly complete models by applying the general approach described below:

1. Choose a previous model as reference
2. Calculate residuals from the previous model applied to the _train_ set
3. Create a new model with an added term that explains the residuals
4. Train parameters on the _test_ set (if applicable)
5. Evaluate performance on the _test_ set

Finally, after development of a complete model resulting from the combination of all the previous steps, we evaluate model performance on the _validation_ set in the Results section of the report.

### Naive model

We begin by constructing a simple model in which the global average rating calculated in the _train_ set (`r mean(ratings$rating)`) is given uniformly as prediction to all ratings.

The model is depicted in mathematical notation below:  

$$Y_{u,m} = \mu + \epsilon$$
where:  
$Y_{u,m}$ = rating given by user $u$ to movie $m$   
$\mu$ = mean global rating    
$\epsilon$ = random residual    

The RMSE result obtained after application of this model to the _test_ set is below:

```{r results for naive model}

results$test %>% 
  filter(modelId == 1) %>% 
  kable(caption = "Results from application of Naive model (modelId = 1) to test set", align = 'c')

```

### Simple movie effect

We proceed to include the movie effect in the model, representing the fact that each movie has an inherent quality that makes it be qualified, on average, above or below the global mean. The movie effect is calculated for each movie as the mean residual obtained after application of the naive model on the _train_ set.
The model with this new parameter is this:

$$Y_{u,m} = \mu + e_{movie} + \epsilon$$   
where:   
$Y_{u,m}$ = rating given by user $u$ to movie $m$   
$\mu$ = mean global rating   
$e_{movie}$ = movie effect for movie $m$  
$\epsilon$ = random residual   

As $e_m$ is modeled as the average points each movie receives above the _train_ set average, $\mu+e_m$ is equal to each individual movie average. This version of the model corresponds to predicting that each movie is rated as the average of previous ratings plus a random term.

Results on the _test_ set are displayed below:

```{r results for simple movie effect model}

results$test %>% 
  filter(modelId == 2) %>% 
  kable(caption = "Results from application of Simple Movie Effect model (modelId = 2) to test set", align = 'c')

```


### Regularized movie effect
 
Although the results did improve, the fact that some movies are rated very few times (as observed in the Exploratory Data Analysis section) indicates that a prediction based solely on the movie average is not adequate. In these situations, it is normally beneficial to use a prediction that is closer to the global average for movies with few ratings, and converges the the movie average as they receive a more substantial number of ratings. A regularization of each movie's mean rating is appropriate to achieve this correction.

The model with the movie effect regularized according to the number of ratings is represented via the equation below:

$$Y_{u,m} = \mu + e_{movie}^r + \epsilon$$   
where:   
$Y_{u,m}$ = rating given by user $u$ to movie $m$  
$\mu$ = mean global rating   
$e_{movie}^r$ = regularized movie effect    
$\epsilon$ = random residual   

The regularized movie effect corresponding to each individual movie is obtained via a modified version of the average deviation from the global average, according to the equation below:

$$e_m^r(\lambda) = \frac{1}{\lambda+n_i}* \sum_{m=1}^{n_i} (Y_{m,i} - \mu)$$
This indicates that the movie effect $e_m^r$ depends on an arbitrary parameter $\lambda$, which can be chosen to minimize the residual.

The figure below depicts the results obtained on the _test_ set after calculating the regularized movie effect on the _train_ set with different values of lambda, from which we choose the optimal.

```{r investigation of lambda for movie effect regularization}

residuals <- 
  predictions$train %>% 
  select(movieId,rating,`global average`) %>% 
  transmute(movieId, residual = rating - `global average`) %>% 
  group_by(movieId) %>% 
  summarise("mean residual" = mean(residual), "number of ratings" = n(), .groups = 'drop') 
  
lambda <- seq(0,10,.1)

R <- sapply(lambda, function(lambda){
  residuals %>% 
    mutate(prediction = parameters$`global average` + `mean residual` * `number of ratings` / (lambda + `number of ratings`)) %>% 
    right_join(select(test,movieId,rating), by = "movieId") %>% 
    summarise(RMSE = RMSE(prediction,rating)) %>% 
    pull(RMSE)
})

df <- 
  data.frame(lambda = lambda, R = R)

df %>% 
  filter(lambda <= 7) %>% 
  ggplot(aes(x = lambda, y = R)) +
  geom_point(data = filter(df, R == min(df$R)), color = 'black', fill = 'red', shape = 21, size = 5) +
  geom_point() +
  labs(title = "RMSE for regularized movie effect with different values of lambda", x = "Lambda", y = "Root Mean Squared Error") +
  annotate(geom = "label", 
           label = paste("lambda = ", df$lambda[which.min(df$R)], "\n RMSE = ", round(min(df$R),5)), 
           x = df$lambda[which.min(df$R)] + 0.7, y = min(df$R) + 0.00005) +
  annotate(geom = "curve", curvature = .1, color = 'red', arrow = arrow(length = unit(2, 'mm')), 
           xend = df$lambda[which.min(df$R)], yend = min(df$R)+0.00001, 
           x = df$lambda[which.min(df$R)] + 0.6, y = min(df$R) + 0.00004)
  
lambda.opt <- df$lambda[which.min(df$R)]

rm(lambda, residuals, R, df)

```

The optimal regularization effect parameter $\lambda$ was found to be `r round(lambda.opt,1)`.

From the RMSE results in the table above, it is evident that regularization has not contributed with a substantial improvement. This may be due to the fact that, as the dataset gets larger, instances of movies with very few ratings become rarer and have a lesser effect on the overall results.

The RMSE after application of this model on the _test_ set is given below:

```{r results for regularized movie effect model}

rm(lambda.opt)

results$test %>% 
  filter(modelId == 3) %>% 
  kable(caption = "Results from application of Regularized Movie Effect model (modelId = 3) to test set", align = 'c')

```

### Simple user effect

Similarly to what was done for the movie averages, we proceed to modify our model to account for the fact that different users tend to rate differently. In order to estimate this effect, we calculate the residuals after application of the previous model on the _train_ set and calculate the average residual for each user. We then add this term for each user to get a new prediction. 

As we include additional terms in the model, it is important to make sure that predictions fall between 0.5 and 5, the minimum and maximum possible ratings. To do so, we introduce the function `limitRange`, defined as below:

$$limitRange_{Y_{min}}^{Y_{max}}(Y) = \begin{cases}Y_{min} &\text{, if } Y < Y_{min} \\ Y_{max} &\text{, if } Y > Y_{max} \\ Y &\text{otherwise } \end{cases}$$

The modified model is depicted below:

$$Y_{u,m} = limitRange_{0.5}^5(\mu + e_{movie}^r + e_{user}+ \epsilon)$$   

where:   
$Y_{u,m}$ = rating given by user $u$ to movie $m$   
$\mu$ = mean global rating   
$e_{movie}^r$ = regularized movie effect   
$e_{user}$ = simple user effect   
$\epsilon$ = random residual   

After application of this model, we obtain the results below:

```{r results for simple user effect model}

results$test %>% 
  filter(modelId == 4) %>% 
  kable(caption = "Results from application of User Effect model (modelId = 4) to test set", align = 'c')

```

### Regularized user effect

As we did for the movie effect, we proceed to modify our model to account for the fact that certain users have a low number of ratings and that their average ratings may be heavily influenced by random variability.

We evaluate several values of $\lambda$ in the _test_ set and select the one that gives the lower value of the RMSE. In this case, the best $\lambda$ is 4.5.

The modified model with the regularized user effect is depicted below:

$$Y_{u,m} = limitRange_{0.5}^5(\mu + e_{movie}^r + e_{user}^r+ \epsilon)$$   

where:   
$Y_{u,m}$ = rating given by user $u$ to movie $m$  
$\mu$ = mean global rating   
$e_{movie}^r$ = regularized movie effect   
$e_{user}^r$ = regularized user effect   
$\epsilon$ = random residual   

The application of this model in the _test_ set gives the results below. We see that regularization does not introduce a great gain in performance, which is explained by the fact the fact that we are using a massive dataset and that ratings from users that are not very active do not account for a significant portion of the total ratings.

```{r results for regularized user effect model}
results$test %>% 
  filter(modelId == 5) %>% 
  kable(caption = "Results from application of Regularized User Effect model (modelId = 5) to test set", align = 'c')
```


### Effect of movie genres (Drama movies)

As observed during the Exploratory Data Analysis section, there is a connection between the movie ratings and the movie genres with which each movie is tagged. However, this connection is already accounted for in the previous model: since there is a one-to-one relation between a movie and it's tags (meaning that the tags are constant and do not change across different ratings), this effect is already represented in the movie average.

Regardless of that, we can investigate the residuals from the previous model to determine whether each individual user is partial do a particular genre. Users with a preference for a particular genre will have higher residuals for movies tagged with that genre.

As a proof of concept, we now create a model with an added term corresponding to the mean residual observed for each user for movies either tagged or not tagged with a particular genre. For this, we choose the Drama genre, which was observed to be the most frequent. We will evaluate the residuals in the _train_ set, calculate their average for each user by grouping movies that are tagged with Drama and movies that are not, and apply regularization by selecting the value of $\lambda$ that minimizes RMSE observed in the _test_ set.

The corresponding model is represented as:

$$Y_{u,m} = limitRange_{0.5}^5(\mu + e_{movie}^r + e_{user}^r+ e_{drama / non-drama}^{r} + \epsilon)$$   

where:   
$Y_{u,m}$ = rating given by user $u$ to movie $m$   
$\mu$ = mean global rating   
$e_{movie}^r$ = regularized movie effect   
$e_{user}^r$ = regularized user effect   
$e_{drama / non-drama}^{r}$ = regularized drama/non-drama effect   
$\epsilon$ = random residual   

To illustrate how this term is distributed across users, we look at a scatter plot of the drama and non-drama effects for a sample of 500 random users:

```{r distribution of drama/non-drama effect}

users %>% 
  sample_n(500) %>% 
  select(userId,`regularized drama effect`, `regularized non-drama effect`) %>% 
  ggplot(aes(x = `regularized drama effect`, `regularized non-drama effect`)) +
  geom_point(alpha = .4, color = 'blue') +
  labs(title = "Regularized drama and non-drama effects for each user", x = "Regularized Drama Effect", y = "Regularized Non-Drama Effect")

```

The plot shows a clear negative correlation between the drama and non-drama effects for each user. This is to be expected, since by definition their average (weighted by the number of drama and non-drama movies) is equal to each user's mean residual.

In order to get a sense of how pronounced the preference is, we look at a plot of the spread between the drama and non-drama effects.

```{r spread between drama and non-drama effects}

spread <-
  users %>% 
  select(userId, `regularized drama effect`, `regularized non-drama effect`) %>% 
  mutate(spread = `regularized drama effect` - `regularized non-drama effect`) %>% 
  pull(spread)

users %>% 
  select(userId, `regularized drama effect`, `regularized non-drama effect`) %>% 
  mutate(spread = `regularized drama effect` - `regularized non-drama effect`) %>% 
  ggplot(aes(x = spread)) +
  geom_polygon(data = data.frame(x = c(mean(spread)-sd(spread),mean(spread)+sd(spread),mean(spread)+sd(spread),mean(spread)-sd(spread)), y = c(-Inf,-Inf,Inf,Inf)),
               mapping = aes(x = x, y = y),
               fill = 'green', alpha = .1) +
  geom_polygon(data = data.frame(x = c(mean(spread)-2*sd(spread),mean(spread)+2*sd(spread),mean(spread)+2*sd(spread),mean(spread)-2*sd(spread)), y = c(-Inf,-Inf,Inf,Inf)),
               mapping = aes(x = x, y = y),
               fill = 'green', alpha = .1) +
  geom_vline(xintercept = mean(spread), color = "blue") +
  geom_density(size = 1) +
  labs(title = "Density plot of difference between drama and non-drama effects", x = "Difference between drama and non-drama effects", y = "Density")

rm(spread)

```

The figure above contains the density plot for the difference between the drama and non-drama effects for each user, with a blue line depicting the average and the different shades of green background depicting the intervals of one and two standard deviations from the mean. The distribution is bell-shaped, centered at 0.008 and with a standard deviation of 0.145. This indicates that, for 95% of users, the difference between the average rating between movies with and without the 'Drama' tag is below 0.29 points.

When we account for this additional piece in the model, we get the following results:

```{r results for drama/non-drama effect model}

results$test %>% 
  filter(modelId == 6) %>% 
  kable(caption = "Results from application of Drama/Non-Drama Effect model (modelId = 6) to test set", align = 'c')

```

### Genre effect through Principal Component Analysis (PCA)

Even though the results obtained with the previous model were an improvement, we would like to extend to analysis for all of the available movie genres. In theory, we could repeat the process of calculating the residuals from one model and adding an effect for each of the movie genres, but that would be repetitive and cumbersome. The most appropriate way to tackle the effect of the movie genres on the ratings is by calculating effects for all of them at the same time.

When addressing the effect of multiple variables at once, we must consider the fact that there is a correlation between the predictors. As observed during the exploratory data analysis, there is a correlation between the tags associated with each of the genres. Consequently, if we choose to calculate effects for each of the genres separately and add them all together according to each movie's genres, we risk "adding the same thing twice".

To account for this correlation, we take the matrix saved in the `genres$wide` object, which contains columns indicating whether each genre tag is part of each of the different genre combinations in the dataset - and perform a Principal Component Analysis (PCA). The resulting object is saved to the `genres$principal.components` object and contains the principal components for the matrix, representing a rotation to a set of coordinates that makes the columns uncorrelated. We can think of principal components as "equivalent genres", which are independent from one another.

The object resulting from the PCA also contains, among other pieces of information, the rotation matrix used to convert from the original logical values indicating whether a movie is tagged with each of the movie genres to the continuous values indicating how much the movie is associated with each of the principal components.

An excerpt from the original data in the `genres$wide` object is shown below:

```{r excerpt of genres$wide}
options(width = 90)
genres$wide %>% as.data.frame() %>% head(6)
```

We can also see an excerpt from the `genres$principal.components$x` object, which contains the values of each of the principal components for each movie in the dataset. The values of the components are continuous, as opposed to the original values which where logical.

```{r excerpt of principal components}
options(width = 90)
genres$principal.components$x %>% head(6)
```

Below we see the output of the `summary()` function applied to the `genres$principal.componenents` object, which has been created using the R function `prcomp()`. The summary depicts how much of the total variance is explained by each of the principal components.

```{r variance explained}
options(width = 90)
genres$principal.components %>% summary()
```

In order to include this information in the model, we begin by grouping all the ratings in the _train_ set by users and performing weighted averages of the residuals of every rating given by each user. These averages are weighted by the values of each of the principal components associated with each movie, so that every user ends up with 20 scores indicating how partial they are to each of the principal components. These values are then regularized, so that results for users with a lower number of ratings have reduced importance. The resulting model follows the equation below:

$$Y_{u,m} = limitRange_{0.5}^5(\mu + e_{movie}^r + e_{user}^r+ \sum_{n = 1}^{N_{PC}}e_{genre(PC)}^r + \epsilon)$$   

where:   
$Y_{u,m}$ = rating given by user $u$ to movie $m$   
$\mu$ = mean global rating   
$e_{movie}^r$ = regularized movie effect   
$e_{user}^r$ = regularized user effect   
$N_{PC}$ = number of principal components   
$e_{genre(PC)}^{r}$ = regularized genre effect     
$\epsilon$ = random residual   

Here, we use all of the 20 principal components of the movie genres tagging system, so $N_{PC}=20$. The value of $\lambda$ that minimizes the residuals after application of this model in the _test_ set is 35, and the RMSE in the _test_ set is displayed below:

```{r results for the genres PCA model}
results$test %>% 
  filter(modelId == 7) %>% 
  kable(caption = "Results from application of Genre Effect model (modelId = 7) to test set", align = 'c')
```
 

### Movie age effect

We now turn our attention to the effect that the year of release (extracted from the movie title in the original dataset) and the date of the rating have on the ratings. As was the case with the movie genres, looking at the year of release in an isolated manner brings no new information, since there is a one-to-one connection between the movie and its age of release. Any effect strictly from the year of release would have already been captured in the movie average rating, already accounted for in previous models.

However, there is one aspect that can still be explored, which is the age of the movie at the time of each rating. We get that by subtracting each movie's year of release from the year each rating was given.

A plot of the average residuals as a function of the movie age at time of rating shows interesting features:

```{r average residuals vs movie age}

predictions$train %>% 
  select(userId,movieId,rating,`regularized genre effect (PCA)`) %>% 
  left_join(select(ratings, userId, movieId, date), by = c("userId","movieId")) %>% 
  left_join(select(movies,movieId,year), by = "movieId") %>% 
  mutate(residual = rating - `regularized genre effect (PCA)`,
         `years since release` = if_else(year(date) - year > 0, year(date) - year, 0)) %>%
  group_by(`years since release`) %>% 
  summarise("average residual" = mean(residual),
            "number of ratings" = n(), .groups = 'drop') %>% 
  ggplot(aes(x = `years since release`, y = `average residual`)) +
  geom_line(color = 'blue', size = 2) +
  scale_x_reverse() +
  labs(title = "Average residuals per years since release", x = "Years since release", y = "Average residual")


```

We can see that the average residual is positive for the first two years after a movie has been release, which seems to indicate some enthusiasm from avid users who like to watch the movies as soon as possible. There is then a period of negative residuals up to around 15 years after release. Finally, the average residual increases until it starts to oscillate around zero, when it appears there is a renewed interest for them (or at least they are considered good enough to be worth watching and rating). For longer periods, the curve gets very noisy, since there are considerably less movies and ratings for these periods.

Because there is a long interval of movie age for which there are few data points, this effect benefits a lot from regularization. The value of $\lambda$ that minimizes the residuals in the _test_ set is 400, considerably larger than for the other phenomenons. 

The model that incorporates this feature of the data is represented below, followed by the results obtained after application on the _test_ set.

$$Y_{u,m} = limitRange_{0.5}^5(\mu + e_{movie}^r + e_{user}^r+ \sum_{n = 1}^{N_{PC}}e_{genre(PC)}^r + e_{age}^{r} + \epsilon)$$   

where:   
$Y_{u,m}$ = rating given by user $u$ to movie $m$   
$\mu$ = mean global rating   
$e_{movie}^r$ = regularized movie effect   
$e_{user}^r$ = regularized user effect   
$N_{PC}$ = number of principal components   
$e_{genre(PC)}^{r}$ = regularized genre effect   
$e_{age}^{r}$ = regularized movie age effect   
$\epsilon$ = random residual   

```{r results from movie age model in test set}
results$test %>% 
  filter(modelId == 8) %>% 
  kable(caption = "Results from application of Movie Age Effect model (modelId = 8) to test set", align = 'c')

```


### Collaborative filtering

As a final step in the modeling phase, we implement a collaborative filtering technique to improve the predictions.

In previous sections, we investigated how each of the predictors related directly with the ratings. Collaborative filtering, on the other hand, looks at how predictors are indirectly related to the ratings. An User-Based Collaborative Filter (UBCF) looks at how similar users have rated a particular movie to come to a prediction. An Item-Based Collaborative Filter (IBCF) looks at how the user has rated similar movies to come to a prediction.

Both approaches required identifying similarities between either different users or different items (movies, in our case).

Here, we prefer to use a IBCF due to the fact that there is a higher concentration of ratings in popular movies than there is in active users, as revealed during the exploratory data analysis phase. This permits some simplifications - instead of calculating similarities between every pair of movies in the dataset, we only calculate similarities between each movie and a selection of the 1000 most popular movies. This approach drastically reduces the dimensions of the matrices containing these similarities.

As a measure of similarity between movies, we choose to calculate the correlation between the residuals obtained after application of the previous model. We calculate the correlation based solely on the users that have rated both movies.

The corrections according to the user's rating to a particular movie is given by the equation below:

$$e_{IBCF}^{u,m} = \frac {M_{u,m_R} * C_{m_R,m}} {\sum_{m_R}^{}C_{m_R,m}}$$
where:   
$e_{IBCF}^{u,m}$ = adjustment to prediction of rating of user $u$ to movie $m$ according to ratings given by user $u$ to similar movies     
$M_{u,m_R}$ = ratings given by user $u$ to set of most popular movies   
$C_{m_R,m}$ = correlation matrix between each movie in the complete dataset and each of the most popular movies, calculated from the set of common users between each pair of movies    
$\sum_{m_R}^{}C_{m_R,m}$ = row sums of $C_{m_R,m}$   

Because the value of $e_{IBCF}^{u,m}$ is unique for each user/movie combination, it is impracticable to represent the full set of values in usual matrix form. The package `Matrix` is used to represent these values in sparse matrix form.

In order to further reduce the need for computational resources, the sets of users and movies are partitioned and the values of $e_{IBCF}^{u,m}$ are calculated sequentially for each combination of this partitions. After some experimentation, we choose to create 7 partitions of users and 10 partitions of movies, so that the full corrections matrix is completed in 70 steps.

With the inclusion of this features, we reach our complete model for predicting movie ratings. It is expressed mathematically as:

$$Y_{u,m} = limitRange_{0.5}^5(\mu + e_{movie}^r + e_{user}^r+ \sum_{n = 1}^{N_{PC}}e_{genre(PC)}^r + e_{age}^{r} + e_{IBCF}^{u,m} + \epsilon)$$   

where:   
$Y_{u,m}$ = rating given by user $u$ to movie $m$  
$\mu$ = mean global rating   
$e_{movie}^r$ = regularized movie effect  
$e_{user}^r$ = regularized user effect   
$N_{PC}$ = number of principal components   
$e_{genre(PC)}^{r}$ = regularized genre effect   
$e_{age}^{r}$ = regularized movie age effect   
$e_{IBCF}^{u,m}$ = adjustment to prediction of rating of user $u$ to movie $m$ according to IBCF   
$\epsilon$ = random residual   

The results after application of this model on the _test_ set are depicted below:   

```{r results for IBCF model}

results$test %>% 
  filter(modelId == 9) %>% 
  kable(caption = "Results from application of Item-Based Collaborative Filter model (modelId = 9) to test set", align = 'c')

```


# Results

The final model has been constructed by considering the rating averages for each particular movie and user. It also considers each user's individual preference for the different movie genres and the effect that the number of years since movie release has on the ratings. Finally, ratings are adjusted according to how each user has rated similar movies (from a list of the 1000 most popular movies). The mathematical representation of the full model is repeated here for convenience:

$$Y_{u,m} = limitRange_{0.5}^5(\mu + e_{movie}^r + e_{user}^r+ \sum_{n = 1}^{N_{PC}}e_{genre(PC)}^r + e_{age}^{r} + \epsilon)$$   

where:   
$Y_{u,m}$ = rating given by user $u$ to movie $m$   
$\mu$ = mean global rating   
$e_{movie}^r$ = regularized movie effect   
$e_{user}^r$ = regularized user effect   
$N_{PC}$ = number of principal components   
$e_{genre(PC)}^{r}$ = regularized genre effect    
$e_{age}^{r}$ = regularized movie age effect   
$\epsilon$ = random residual   

The results obtained as we applied each step in the modeling phase to the _test_ set were presented in their respective sections. Here, we aggregate the results from all versions of model after application to the _train_, _test_, and _validation_ sets:

```{r full results in all 3 sets}

options(width = 90)

data.frame(modelId = results$train$modelId,
           results_train = results$train$RMSE,
           results_test = results$test$RMSE,
           results_validation = results$validation$RMSE,
           description = results$train$description) %>% 
  kable(col.names = c("modelId", "RMSE (train set)", "RMSE (test set)", "RMSE (validation set)", "Description"), align = 'c')

```

We see from the table that the results obtained on the _validation_ set are very close to the ones obtained on the _test_ set, which indicates that the validation strategy (partitioninig the `edx` dataset into a _train_ and a _test_ sets) was appropriate and that the tuning parameters defined for each aspect of the model are robust and not subject to overtraining. The results on the _train_ set are considerably better, which does indicate that they are not representative of what should be expected when exposing the model to new datasets.

We also see from the results that the improvement to the RMSE as new effects are incorporated into the model are very subtle and seem to converge to around 0.84. This reflects the fact the ratings have an intrinsic level of random variability and that there is a limit to the level of precision that any model can attain.

However, this subtle improvements to the RMSE should not be interpreted as insignificant performance improvements in practice. Ultimately, the goal of these models is not to predict ratings, but rather to create movie recommendations for each particular user. Because the objective is to use the models to create an ordered list of movie recommendations, a small change in the predicted ratings can have a big impact in the order at which movies are recommended.

The table contains the top 20 recommendations given to a randomly selected user by 3 of the models and illustrates this feature:

```{r recommendations for random user}

options(width = 90)

random.user <-
  users %>%
  filter(userId %in% predictions$`sample users`$userId) %>%
  top_n(n = 1, wt = `number of ratings`) %>%
  pull(userId)

random.user.predictions <-
  predictions$`sample users` %>%
  filter(userId == random.user) %>%
  left_join(select(movies,movieId,title), by = "movieId")

data.frame(model5 = random.user.predictions$title[order(random.user.predictions$`regularized user effect`,decreasing = TRUE)],
           model7 = random.user.predictions$title[order(random.user.predictions$`regularized genre effect (PCA)`,decreasing = TRUE)],
           model9 = random.user.predictions$title[order(random.user.predictions$`similar movies effect`,decreasing = TRUE)]) %>%
  head(20) %>%
  kable(caption = "Movie recommendations from 3 models to a random user",
        col.names = c("Model 5: regularized user effect", "Model 7: regularized genre effect", "Model 9 (complete model): similar movies effect"))

rm(random.user, random.user.predictions)

```

Finally, the result of the RMSE after application of the complete model to the _validation_ set is approximately 0.843.

***

# Conclusion

This project covered the construction of a statistical model to predict movie ratings. A _train_ set of approximately 7.2 million ratings was used to create predictions with parameters tuned with a _test_ set of approximately 1.8 million ratings. Finally, model performance was measured on a _validation_ set with approximately 1 million ratings. The Root Mean Squared Error (RMSE) was used as a metric, and a RMSE of 0.843 was reached with the complete model in the _validation_ set.

Models were created in the Modeling section based on trends identified in the Exploratory Data Analysis section. Increasingly complex models were created by including terms that accounted for specific trends present in the residuals from the previous model.

In a practical setting, the results of the prediction models would be used to predict ratings to movies that have not already been watched/rated by a particular user. Movies would be ordered according to their predicted ratings and recommended to users. In this sense, the prediction and its RMSE are not the actual outcome of the model. Instead, a proper outcome would be a list of recommended movies.

Considering this fact, a list of recommendations generated by some of the models for a random user was also presented in the Results section. We notice that, even though the RMSE decreases very slightly with each step in the modeling process, the list of recommended movies changes more drastically.

One point not taken into consideration during the project is the fact that, in a practical setting, there are sources of "voting" other than the ratings given by users. Many times, users do not go through the trouble of rating a movie they've just watched. User are also not very consistent in givin ratings, and there is an intrinsic random component to a rating. A potentially more illustrative indication of an user's preferences may be the list of movies he or she have watched, regardless of ratings.

We also note that the strategy for partitioning the full set of ratings may be somewhat misleading. We randomly selected proportions of the original _MovieLens 10M_ dataset, but a more relevant manner of partitioning the dataset would be to consider the date of rating. For instance, we could have set apart ratings from the last year in the dataset for validation. With this approach, we wouldn't be using future ratings to predict past ratings.

A potential point of improvement would be to perform further training on the collaborative filter portion of the model. The number of movies in the set of popular movies, used as reference for calculating a matrix of correlations and identifying similar movies was chosen from limited experimentation with a few set of values. The sizes of the users and movies partitions were also not fully explored to produce results with the lowest RMSE.

This was mostly due to the high demand for computational power. Calculating the predictions for one single set of values took nearly one hour in a home computer, so performing training with a large set of parameters would have been impractical. One possible solution would be to make use of the parallel computing capabilities in the `caret` package, which allows for the evaluation of performance for different combinations of parameters to be evaluated in parallel using additional processor cores.